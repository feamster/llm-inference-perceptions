@inproceedings{synthpai,
author = {Yukhymenko, Hanna and Staab, Robin and Vero, Mark and Vechev, Martin},
title = {A synthetic dataset for personal attribute inference},
year = {2025},
isbn = {9798331314385},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently powerful Large Language Models (LLMs) have become easily accessible to hundreds of millions of users world-wide. However, their strong capabilities and vast world knowledge do not come without associated privacy risks. In this work, we focus on the emerging privacy threat LLMs pose - the ability to accurately infer personal information from online texts. Despite the growing importance of LLM-based author profiling, research in this area has been hampered by a lack of suitable public datasets, largely due to ethical and privacy concerns associated with real personal data. We take two steps to address this problem: (i) we construct a simulation framework for the popular social media platform Reddit using LLM agents seeded with synthetic personal profiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic dataset of over 7800 comments manually labeled for personal attributes. We validate our dataset with a human study showing that humans barely outperform random guessing on the task of distinguishing our synthetic comments from real ones. Further, we verify that our dataset enables meaningful personal attribute inference research by showing across 18 state-of-the-art LLMs that our synthetic comments allow us to draw the same conclusions as real-world data. Combined, our experimental results, dataset and pipeline form a strong basis for future privacy-preserving research geared towards understanding and mitigating inference-based privacy threats that LLMs pose.},
booktitle = {Proceedings of the 38th International Conference on Neural Information Processing Systems},
articleno = {3837},
numpages = {45},
location = {Vancouver, BC, Canada},
series = {NIPS '24}
}

@inproceedings{rescriber,
author = {Zhou, Jijie and Xu, Eryue and Wu, Yaoyao and Li, Tianshi},
title = {Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based Chatbots},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713701},
doi = {10.1145/3706598.3713701},
abstract = {The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users’ personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=Rescriber) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users’ subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4o. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users’ trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {246},
numpages = {28},
keywords = {privacy, security, LLM, AI, chatbot, PII, ChatGPT},
location = {
},
series = {CHI '25}
}

@misc{bertscore,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

@misc{li2024humancenteredprivacyresearchage,
      title={Human-Centered Privacy Research in the Age of Large Language Models}, 
      author={Tianshi Li and Sauvik Das and Hao-Ping Lee and Dakuo Wang and Bingsheng Yao and Zhiping Zhang},
      year={2024},
      eprint={2402.01994},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2402.01994}, 
}

@inproceedings{windl2022contextualprivacy, author = {Windl, Maximiliane and Henze, Niels and Schmidt, Albrecht and Feger, Sebastian S.}, title = {Automating Contextual Privacy Policies: Design and Evaluation of a Production Tool for Digital Consumer Privacy Awareness}, year = {2022}, isbn = {9781450391573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491102.3517688}, doi = {10.1145/3491102.3517688}, booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems}, articleno = {34}, numpages = {18}, keywords = {contextual privacy, online services, privacy, privacy policies}, location = {New Orleans, LA, USA}, series = {CHI '22} }

@article{nissenbaum2004privacy,
  title={Privacy as contextual integrity},
  author={Nissenbaum, Helen},
  journal={Wash. L. Rev.},
  volume={79},
  pages={119},
  year={2004},
  publisher={HeinOnline}
}

@article{WOODRING2024103997,
title = {Enhancing privacy policy comprehension through Privacify: A user-centric approach using advanced language models},
journal = {Computers \& Security},
volume = {145},
pages = {103997},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103997},
url = {https://www.sciencedirect.com/science/article/pii/S016740482400302X},
author = {Justin Woodring and Katherine Perez and Aisha Ali-Gombe},
keywords = {Privacy, LLM, Privacy policy, Summarization, Machine learning, Software development, NLP, GPT},
}

@article{irr,
author = {McDonald, Nora and Schoenebeck, Sarita and Forte, Andrea},
title = {Reliability and Inter-rater Reliability in Qualitative Research: Norms and Guidelines for CSCW and HCI Practice},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359174},
doi = {10.1145/3359174},
abstract = {What does reliability mean for building a grounded theory? What about when writing an auto-ethnography? When is it appropriate to use measures like inter-rater reliability (IRR)? Reliability is a familiar concept in traditional scientific practice, but how, and even whether to establish reliability in qualitative research is an oft-debated question. For researchers in highly interdisciplinary fields like computer-supported cooperative work (CSCW) and human-computer interaction (HCI), the question is particularly complex as collaborators bring diverse epistemologies and training to their research. In this article, we use two approaches to understand reliability in qualitative research. We first investigate and describe local norms in the CSCW and HCI literature, then we combine examples from these findings with guidelines from methods literature to help researchers answer questions like: "should I calculate IRR?" Drawing on a meta-analysis of a representative sample of CSCW and HCI papers from 2016-2018, we find that authors use a variety of approaches to communicate reliability; notably, IRR is rare, occurring in around 1/9 of qualitative papers. We reflect on current practices and propose guidelines for reporting on reliability in qualitative research using IRR as a central example of a form of agreement. The guidelines are designed to generate discussion and orient new CSCW and HCI scholars and reviewers to reliability in qualitative research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {72},
numpages = {23},
keywords = {IRR, content analysis, inter-rater reliability, interviews, qualitative methods}
}


@article{li2024vldb,
author = {Li, Qinbin and Hong, Junyuan and Xie, Chulin and Tan, Jeffrey and Xin, Rachel and Hou, Junyi and Yin, Xavier and Wang, Zhun and Hendrycks, Dan and Wang, Zhangyang and Li, Bo and He, Bingsheng and Song, Dawn},
title = {LLM-PBE: Assessing Data Privacy in Large Language Models},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681994},
doi = {10.14778/3681954.3681994},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3201–3214},
numpages = {14}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX security symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@inproceedings{lukas2023analyzing,
  title={Analyzing leakage of personally identifiable information in language models},
  author={Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-B{\'e}guelin, Santiago},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={346--363},
  year={2023},
  organization={IEEE}
}

@INPROCEEDINGS{carlini2022membership,
  author={Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramèr, Florian},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
  title={Membership Inference Attacks From First Principles}, 
  year={2022},
  volume={},
  number={},
  pages={1897-1914},
  keywords={Measurement;Training;Privacy;Differential privacy;Computational modeling;Training data;Machine learning},
  doi={10.1109/SP46214.2022.9833649}}

@misc{carlini2023quantifying,
      title={Quantifying Memorization Across Neural Language Models}, 
      author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
      year={2023},
      eprint={2202.07646},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.07646}, 
}

@INPROCEEDINGS {shokri2017membership,
author = { Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly },
booktitle = { 2017 IEEE Symposium on Security and Privacy (SP) },
title = {{ Membership Inference Attacks Against Machine Learning Models }},
year = {2017},
volume = {},
ISSN = {2375-1207},
pages = {3-18},
keywords = {Training;Data models;Predictive models;Privacy;Sociology;Statistics;Google},
doi = {10.1109/SP.2017.41},
url = {https://doi.ieeecomputersociety.org/10.1109/SP.2017.41},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =May}


@inproceedings{mireshghallah-etal-2022-quantifying,
    title = "Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks",
    author = "Mireshghallah, Fatemehsadat  and
      Goyal, Kartik  and
      Uniyal, Archit  and
      Berg-Kirkpatrick, Taylor  and
      Shokri, Reza",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.570/",
    doi = "10.18653/v1/2022.emnlp-main.570",
    pages = "8332--8347",
}

@article{inan2021training,
  title={Training data leakage analysis in language models},
  author={Inan, Huseyin A and Ramadan, Osman and Wutschitz, Lukas and Jones, Daniel and R{\"u}hle, Victor and Withers, James and Sim, Robert},
  journal={arXiv preprint arXiv:2101.05405},
  year={2021}
}

@Article{Parikh2022,
 author = {Rahil Parikh and Christophe Dupuy and Rahul Gupta},
 title = {Canary extraction in natural language understanding models},
 year = {2022},
 url = {https://www.amazon.science/publications/canary-extraction-in-natural-language-understanding-models},
}

@article{mccoy-etal-2023-much,
    title = "How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using {RAVEN}",
    author = "McCoy, R. Thomas  and
      Smolensky, Paul  and
      Linzen, Tal  and
      Gao, Jianfeng  and
      Celikyilmaz, Asli",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.38/",
    doi = "10.1162/tacl_a_00567",
    pages = "652--670",
}

@inproceedings{zanella2020ccs,
author = {Zanella-B\'{e}guelin, Santiago and Wutschitz, Lukas and Tople, Shruti and R\"{u}hle, Victor and Paverd, Andrew and Ohrimenko, Olga and K\"{o}pf, Boris and Brockschmidt, Marc},
title = {Analyzing Information Leakage of Updates to Natural Language Models},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417880},
doi = {10.1145/3372297.3417880},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {363–375},
numpages = {13},
keywords = {privacy, neural networks, natural language, machine learning},
location = {Virtual Event, USA},
series = {CCS '20}
}

@article{crothers2023machine,
  title={Machine-generated text: A comprehensive survey of threat models and detection methods},
  author={Crothers, Evan N and Japkowicz, Nathalie and Viktor, Herna L},
  journal={IEEE Access},
  volume={11},
  pages={70977--71002},
  year={2023},
  publisher={IEEE}
}

@inproceedings{li-etal-2023-multi-step,
    title = "Multi-step Jailbreaking Privacy Attacks on {C}hat{GPT}",
    author = "Li, Haoran  and
      Guo, Dadi  and
      Fan, Wei  and
      Xu, Mingshi  and
      Huang, Jie  and
      Meng, Fanpu  and
      Song, Yangqiu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.272/",
    doi = "10.18653/v1/2023.findings-emnlp.272",
    pages = "4138--4153",
}

@article{perez2022ignore,
  title={Ignore previous prompt: Attack techniques for language models},
  author={Perez, F{\'a}bio and Ribeiro, Ian},
  journal={arXiv preprint arXiv:2211.09527},
  year={2022}
}

@article{das2025security,
  title={Security and privacy challenges of large language models: A survey},
  author={Das, Badhan Chandra and Amini, M Hadi and Wu, Yanzhao},
  journal={ACM Computing Surveys},
  volume={57},
  number={6},
  pages={1--39},
  year={2025},
  publisher={ACM New York, NY}
}

@article{liu2023jailbreaking,
  title={Jailbreaking chatgpt via prompt engineering: An empirical study},
  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Wang, Kailong and Liu, Yang},
  journal={arXiv preprint arXiv:2305.13860},
  year={2023}
}

@inproceedings{Siwon2023propile,
author = {Kim, Siwon and Yun, Sangdoo and Lee, Hwaran and Gubri, Martin and Yoon, Sungroh and Oh, Seong Joon},
title = {ProPILE: probing privacy leakage in large language models},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {911},
numpages = {13},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{huang-etal-2022-large,
    title = "Are Large Pre-Trained Language Models Leaking Your Personal Information?",
    author = "Huang, Jie  and
      Shao, Hanyin  and
      Chang, Kevin Chen-Chuan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.148/",
    doi = "10.18653/v1/2022.findings-emnlp.148",
    pages = "2038--2047",
}

@article{nakamura2020kart,
  title={Kart: Privacy leakage framework of language models pre-trained with clinical records},
  author={Nakamura, Yuta and Hanaoka, Shouhei and Nomura, Yukihiro and Hayashi, Naoto and Abe, Osamu and Yada, Shuntaro and Wakamiya, Shoko and Aramaki, Eiji},
  journal={arXiv preprint arXiv:2101.00036},
  year={2020}
}

@article{nakka2024pii,
  title={PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding},
  author={Nakka, Krishna Kanth and Frikha, Ahmed and Mendes, Ricardo and Jiang, Xue and Zhou, Xuebing},
  journal={arXiv preprint arXiv:2407.02943},
  year={2024}
}

@article{tomekcce2024private,
  title={Private attribute inference from images with vision-language models},
  author={T{\"o}mek{\c{c}}e, Batuhan and Vero, Mark and Staab, Robin and Vechev, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={103619--103651},
  year={2024}
}

@inproceedings{
staab2024beyond,
title={Beyond Memorization: Violating Privacy via Inference with Large Language Models},
author={Robin Staab and Mark Vero and Mislav Balunovic and Martin Vechev},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=kmn0BhQk7p}
}

@article{
kosinski2013private,
author = {Michal Kosinski  and David Stillwell  and Thore Graepel },
title = {Private traits and attributes are predictable from digital records of human behavior},
journal = {Proceedings of the National Academy of Sciences},
volume = {110},
number = {15},
pages = {5802-5805},
year = {2013},
doi = {10.1073/pnas.1218772110},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1218772110},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1218772110},
}

@article{weidinger2021ethical,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}

@online{brewster2023chatgpt,
  author = {Brewster, Thomas},
  title = {The ChatGPT Effect: How An A.I. Is Now Being Used To Spy On Social Media And Siphon Information},
  year = {2023},
  month = {November},
  day = {16},
  url = {https://www.forbes.com/sites/thomasbrewster/2023/11/16/chatgpt-becomes-a-social-media-spy-assistant/},
  organization = {Forbes},
  note = {Accessed: [Current Date, e.g., Aug. 26, 2025]}
}

@inproceedings{dou-etal-2024-reducing,
    title = "Reducing Privacy Risks in Online Self-Disclosures with Language Models",
    author = "Dou, Yao  and
      Krsek, Isadora  and
      Naous, Tarek  and
      Kabra, Anubha  and
      Das, Sauvik  and
      Ritter, Alan  and
      Xu, Wei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.741/",
    doi = "10.18653/v1/2024.acl-long.741",
    pages = "13732--13754",
}

@inproceedings{estival2007author,
  title={Author profiling for English emails},
  author={Estival, Dominique and Gaustad, Tanja and Pham, Son Bao and Radford, Will and Hutchinson, Ben},
  booktitle={Proceedings of the 10th conference of the Pacific Association for computational linguistics},
  volume={263},
  pages={272},
  year={2007}
}

@article{mireshghallah2024trust,
  title={Trust no bot: Discovering personal disclosures in human-llm conversations in the wild},
  author={Mireshghallah, Niloofar and Antoniak, Maria and More, Yash and Choi, Yejin and Farnadi, Golnoosh},
  journal={arXiv preprint arXiv:2407.11438},
  year={2024}
}

@inproceedings{zhang2024s,
  title={“It's a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents},
  author={Zhang, Zhiping and Jia, Michelle and Lee, Hao-Ping and Yao, Bingsheng and Das, Sauvik and Lerner, Ada and Wang, Dakuo and Li, Tianshi},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  pages={1--26},
  year={2024}
}

@inproceedings{malki2025hoovered,
  title={``Hoovered up as a data point'': Exploring Privacy Behaviours, Awareness, and Concerns Among UK Users of LLM-based Conversational Agents},
  author={Malki, Lisa Mekioussa and others},
  booktitle={Proceedings on Privacy Enhancing Technologies},
  year={2025},
  organization={ACM}
}

@inproceedings{ischen2019privacy,
  title={Privacy concerns in chatbot interactions},
  author={Ischen, Carolin and Araujo, Theo and Voorveld, Hilde and Van Noort, Guda and Smit, Edith},
  booktitle={International workshop on chatbot research and design},
  pages={34--48},
  year={2019},
  organization={Springer}
}

@inproceedings{stock2023tell,
  title={Tell me, what are you most afraid of? Exploring the effects of agent representation on information disclosure in human-chatbot interaction},
  author={Stock, Anna and Schl{\"o}gl, Stephan and Groth, Aleksander},
  booktitle={International Conference on Human-Computer Interaction},
  pages={179--191},
  year={2023},
  organization={Springer}
}

@report{DSIT2024PublicAttitudes,
  author = {{Department for Science, Innovation and Technology} and {Feryal Clark MP}},
  title = {Public attitudes to data and AI: Tracker survey (Wave 4) report},
  year = {2024},
  month = {December},
  url = {https://www.gov.uk/government/publications/public-attitudes-to-data-and-ai-tracker-survey-wave-4/public-attitudes-to-data-and-ai-tracker-survey-wave-4-report},
  institution = {GOV.UK},
  note = {Accessed: [Current Date, e.g., Aug. 26, 2025]}
}

@inproceedings{kimbel2024security,
  title={Security and Privacy Perspectives on Using ChatGPT at the Workplace: An Interview Study},
  author={Kimbel, Angelika and Glas, Magdalena and Pernul, G{\"u}nther},
  booktitle={International Symposium on Human Aspects of Information Security and Assurance},
  pages={184--197},
  year={2024},
  organization={Springer}
}

@inproceedings{chalhoub2020alexa,
  title={“Alexa, are you spying on me?”: Exploring the Effect of User Experience on the Security and Privacy of Smart Speaker Users},
  author={Chalhoub, George and Flechais, Ivan},
  booktitle={International conference on human-computer interaction},
  pages={305--325},
  year={2020},
  organization={Springer}
}

@inproceedings{chametka2023security,
  title={Security and privacy perceptions of mental health chatbots},
  author={Chametka, Paulina and Maqsood, Sana and Chiasson, Sonia},
  booktitle={2023 20th Annual International Conference on Privacy, Security and Trust (PST)},
  pages={1--7},
  year={2023},
  organization={IEEE}
}

@inproceedings{belen2021privacy,
  title={Privacy concerns in chatbot interactions: When to trust and when to worry},
  author={Belen Saglam, Rahime and Nurse, Jason RC and Hodges, Duncan},
  booktitle={International Conference on Human-Computer Interaction},
  pages={391--399},
  year={2021},
  organization={Springer}
}

@inproceedings{liu2025prevalence,
  title={Prevalence Overshadows Concerns? Understanding Chinese Users' Privacy Awareness and Expectations Towards LLM-Based Healthcare Consultation},
  author={Liu, Zhihuang and Hu, Ling and Zhou, Tongqing and Tang, Yonghao and Cai, Zhiping},
  booktitle={2025 IEEE Symposium on Security and Privacy (SP)},
  pages={2716--2734},
  year={2025},
  organization={IEEE}
}

@article{alawida2024unveiling,
  title={Unveiling the dark side of chatgpt: Exploring cyberattacks and enhancing user awareness},
  author={Alawida, Moatsum and Abu Shawar, Bayan and Abiodun, Oludare Isaac and Mehmood, Abid and Omolara, Abiodun Esther and Al Hwaitat, Ahmad K},
  journal={Information},
  volume={15},
  number={1},
  pages={27},
  year={2024},
  publisher={MDPI}
}

@article{alkamli2024understanding,
  title={Understanding privacy concerns in ChatGPT: A data-driven approach with LDA topic modeling},
  author={Alkamli, Shahad and Alabduljabbar, Reham},
  journal={Heliyon},
  volume={10},
  number={20},
  year={2024},
  publisher={Elsevier}
}

@misc{ChatGPTStats,
  author       = {Arooj Ahmed},
  title        = {ChatGPT Usage Statistics: Numbers Behind Its Worldwide Growth and Reach},
  howpublished = {Digital Information World},
  year         = {2025},
  month        = {May 16},
  note         = {Last updated August 10, 2025; accessed 2025-09-11},
  url          = {https://www.digitalinformationworld.com/2025/05/chatgpt-stats-in-numbers-growth-usage-and-global-impact.html}
}

@inproceedings{truth2021,
author = {Ramokapane, Kopo M. and Misra, Gaurav and Such, Jose and Preibusch, S\"{o}ren},
title = {Truth or Dare: Understanding and Predicting How Users Lie and Provide Untruthful Data Online},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445625},
doi = {10.1145/3411764.3445625},
abstract = {Individuals are known to lie and/or provide untruthful data when providing information online as a way to protect their privacy. Prior studies have attempted to explain when and why individuals lie online. However, no work has examined into how people lie or provide untruthful data online, i.e. the specific strategies they follow to provide untruthful data, or attempted to predict whether people would be truthful or not depending on the specific question/data. To close this gap, we present a large-scale study with over 800 participants. Based on it, we show that it is possible to predict whether users are truthful or not using machine learning with very high accuracy (89.7\%). We also identify four main strategies people employ to provide untruthful data and show the factors that influence the choices of their strategies. We discuss the implications of findings and argue that understanding privacy lies at this level can help both users and data collectors.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {557},
numpages = {15},
keywords = {Untruthful data, Privacy lies, Privacy Protective Behaviors, False Information},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{li2020,
author = {Li, Xiao‐Bai and Liu, Xiaoping and Motiwalla, Luvai},
year = {2020},
month = {05},
pages = {},
title = {Valuing Personal Data with Privacy Consideration},
volume = {52},
journal = {Decision Sciences},
doi = {10.1111/deci.12442}
}

@article{lee2021digital,
  title={Digital inequality through the lens of self-disclosure},
  author={Lee, Jooyoung and Rajtmajer, Sarah and Srivatsavaya, Eesha and Wilson, Shomir},
  journal={Proceedings on Privacy Enhancing Technologies},
  year={2021}
}

@inproceedings{zhang2025through,
  title={Through their eyes: User perceptions on sensitive attribute inference of social media videos by visual language models},
  author={Zhang, Shuning and Zhang, Gengrui and Meng, Yibo and Zhang, Ziyi and Zhao, Hantao and Yi, Xin and Li, Hewu},
  booktitle={Proceedings of the 2025 Workshop on Human-Centered AI Privacy and Security},
  pages={20--32},
  year={2025}
}

@inproceedings{bi2023create,
  title={I create, therefore I agree: Exploring the effect of AI anthropomorphism on human decision-making},
  author={Bi, Nanyi and Huang, Janet Yi-Ching},
  booktitle={Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
  pages={241--244},
  year={2023}
}

@article{shim2024unveiling,
  title={Unveiling secrets to AI agents: Exploring the interplay of conversation type, self-disclosure, and privacy insensitivity},
  author={Shim, Hongjin and Cho, Jaeho and Sung, Yoon Hi},
  journal={Asian Communication Research},
  volume={21},
  number={2},
  pages={195--216},
  year={2024}
}

@article{choi2025privacy,
  title={Privacy disclosure to large language models: A large-scale study on awareness, benefits, and concerns in health contexts across three countries},
  author={Choi, Yubin and Zhunis, Assem and Dong, Wenchao and Seering, Joseph and Park, Sangchul and Cha, Meeyoung and Chin, Hyojin},
  journal={Computers in Human Behavior Reports},
  volume={20},
  pages={100841},
  year={2025},
  publisher={Elsevier}
}

@article{peter2025benefits,
  title={The benefits and dangers of anthropomorphic conversational agents},
  author={Peter, Sandra and Riemer, Kai and West, Jevin D},
  journal={Proceedings of the National Academy of Sciences},
  volume={122},
  number={22},
  pages={e2415898122},
  year={2025},
  publisher={National Academy of Sciences}
}

@inproceedings{cohn2024believing,
  title={Believing anthropomorphism: examining the role of anthropomorphic cues on trust in large language models},
  author={Cohn, Michelle and Pushkarna, Mahima and Olanubi, Gbolahan O and Moran, Joseph M and Padgett, Daniel and Mengesha, Zion and Heldreth, Courtney},
  booktitle={Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2024}
}

@inproceedings{jo2024understanding,
  title={Understanding the impact of long-term memory on self-disclosure with large language model-driven chatbots for public health intervention},
  author={Jo, Eunkyung and Jeong, Yuin and Park, SoHyun and Epstein, Daniel A and Kim, Young-Ho},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}
