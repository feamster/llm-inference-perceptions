\section{Findings}
In this section, we will go through our findings in detail. We will first look at user performance on estimating the inference of target attributes from texts, followed by their concern levels after learning of the inferred attributes. Finally, we will present the evaluation of user rewrites.

\subsection{User Estimation of Attribute Inference}
\label{sec:user_estimation}
\input{fig/matrix}
As shared earlier in Section~\ref{sec:survey_questions}, participants are shown a text and asked to select which of the eight private attributes are inferable from the text by LLMs. Remember that only one attribute is inferable from each text (due to our selection of texts as discussed in Section~\ref{sec:dataset}), but participants are not given this information and are allowed to select more than one attribute. Figure~\ref{fig:matrix} summarizes their selections. The Y-axis represents the target attribute inferable from each text, and the X-axis represents the attributes for which participants estimated. 
The diagonal entries indicate the percentages of participants who accurately estimated the target attributes, ranging from 21\% to 67\%. Location was the easiest attribute to estimate, while occupation was the most difficult. 
%All target attributes received the most positive estimation with the exception of occupation and place of birth. 
Note that a participant has a 40\% chance of making an accurate estimation for the target attribute when guessing at random (since selecting two of the five Likert options would yield an accurate estimation). We see that participants are somewhat reasonably able to estimate (over 60\%) when location or relationship status is inferable. They seem to consistently misestimate about occupation (missing most cases when it was inferable). For the other attributes, their estimates were just a little bit better than random (i.e.,  in the 42-50\% range).

%Only location and relationship status attributes received more than \nina{removr 10 percent  comment, replace with for location and relationship, users get it right reassonably often 60 percent of the time, fair bit better than random.}10\% accurate estimations from participants compared to random guessing. Occupation, on the other hand, received accuracy estimates far less than a random guess. This implies participants often think occupation is not inferable even when it is.\nina{tell story breaking it down by attribute, do ok for location rel, consistently wrong for occuptaion, and random for others}

Off-diagonal entries represent false positives, where the user overestimates LLMs' ability to predict an attribute. These overestimates vary from 5\% to 57\%. Two specific scenarios stand out when looking at these overestimates: when the target attribute is occupation, age received the most positive estimations from participants; and when the target attribute is place of birth, location received the most positive estimations. While it is hard to postulate the reason for occupation and age overestimates, we can see a semantic overlap between location and place of birth attributes -- both are geographic locations. Since place of birth is a type of location, participants frequently selected location as being inferable for these texts, resulting in the overestimates. Additionally, it is interesting that participant overestimates are directional in nature, i.e., we do not see the same number of overestimates for place of birth when the target attribute is location, potentially because there are other types of locations beyond place of birth. Also, we acknowledge (as mentioned in Section~\ref{sec:limiation} and further discussed in Section ~\ref{subsec:discuss-awareness}) that asking participants to guess which attributes are inferable (when they are aware that some are inferable) might encourage them to seek patterns and overselect. However, this seems unavoidable given the specific question we study. 

\input{fig/score_distributions}

We computed both \textit{score} and \textit{weighted score} for each participant (described in Section~\ref{sec:user_estimation_methods}). 
As shown in Figure~\ref{fig:score_distribution}, most participants scored 0.5, corresponding to making accurate estimations for target attributes for two out of four questions. Only 8\% of participants identified all inferable target attributes correctly, whereas 14\% missed all four. 
The weighted score distribution (that reflects accurate estimations across both target and non-target attributes) of the participants is shown in Figure~\ref{fig:weighted_score_distribution}. While there were participants who were mostly accurate (with weighted scores $>0.875$), none of them correctly predicted all target and non-target attributes for all four questions. Only 67 participants (28\%) made correct predictions for both target and non-target attributes in at least one question. These weighted scores also demonstrate that participants frequently overestimated the capabilities of LLMs in inferring privacy attributes from texts.

Figure~\ref{fig:score_distribution} and~\ref{fig:weighted_score_distribution} also show the expected distribution of scores with random guessing. Overall, participants made more accurate estimations than expected from random guessing, as reflected in the participant distribution histogram shifting to the right relative to the random guessing line plot. Participant average score is 0.48 and average weighted score is 0.58, which are 20\% and 45\% over the 0.4 expectation from random guessing for both score and weighted score. The higher weighted score indicates that participants are often successful at narrowing down which attributes are inferable, getting extra points from correctly eliminating non-inferable attributes.

Finally, we tested for correlations between participant scores and how frequently they use any LLMs and their demographic information (their age, gender, and education level), but found no correlation. Categories under each type of information were used as variables, and we compared the distribution of scores (count of participants who got each possible score) using the Chi-Square Test of Independence. P-values for age, gender, and education level are 0.4471, 0.4889, 0.8388, and 0.3054, respectively. For the score distributions across different demographic and LLM usage variables, see Figure~\ref{fig:score_by_usage},~\ref{fig:score_by_age},~\ref{fig:score_by_gender}, and~\ref{fig:score_by_education} in Appendix~\ref{app:additional_analysis}.


\subsection{User Concern About Attribute Inference}\label{sec:user_concern}
\input{fig/concern_levels}
28\% of participants were not at all concerned about all four attributes they were asked about, and 44\% of participants were concerned about (slightly concerned or more) all four attributes. Figure~\ref{fig:cl_by_attribute} shows the distribution of concern levels by attribute type. We computed the pairwise Kullback-Leibler (KL) divergence metric for the concern distributions of each attribute pair, and found that all the divergence values were below 0.1, showing that the difference between concern levels across attributes is not significant.


\subsection{Evaluating Rewrite Strategies}
To evaluate the effectiveness of participant rewrites in preventing attribute inference, we first identify whether the inference is still possible from the rewrites. To benchmark participants' performance with online tools, we also evaluate rewrites from ChatGPT and the Rescriber data sanitization tool. 
Table~\ref{table:rewrite_success} shows the effectiveness of rewrites for our participants, as well as ChatGPT and Rescriber.


%\input{fig/success_rate}

\subsubsection{Effectiveness}

\input{tables/rewrite_success}

As shown in Table~\ref{table:rewrite_success}, ChatGPT demonstrated the highest effectiveness, successfully rewriting 50\% of texts. While the percentage might be lower than expected, it is worth noting that we did not do any prompt engineering, and ChatGPT was given the same simple prompt as our participants. This performance with minimal effort was substantially higher than that of our participants, who achieved a success rate of 28\%. Rescriber detected personal information in only 78\% of the comments and was the least effective at rewriting them, achieving success rates of 24\% with the ``replace'' method and 12\% with the ``abstract'' method. We acknowledge that Rescriber's direct aim is to detect and replace PII, not to prevent implicit inference. Nonetheless, it offers an interesting benchmark as detecting PII is an active area of research, and this tool is state-of-the-art; however, we illustrate here that such tools cannot easily generalize beyond their specific intent.

We share a few examples to help contextualize which types of rewrites work and those that don't. 
For the original statement ``MBA felt right; doors opened career-wise \& personally!'' with inferred attribute `Education' being `Masters in Business Administration', one participant rewrote this as ``Getting my degree felt right; doors opened career-wise \& personally!''. Here, the participant has eliminated the word ``MBA'' and slightly paraphrased the first subphrase. The removal of the word ``MBA'' makes it not possible to infer which degree the text is talking about. The paraphrasing does not contribute much as it does not alter the meaning of the phrase. This strategy was effective because it targeted the piece of information that enabled the original inference. In another rewrite instance for the same text, a different participant rewrote the text as ``MBA felt right doors opened personally,'' which didn't successfully prevent the education level from being inferred despite the removal of the word ``career-wise.'' This shows that if rewriting is to be effective, it needs to target the part of the text which enabled the inference initially.

\input{tables/rewrite_examples}

Next, we compare ChatGPT and user rewrites. In particular, we compare examples where one is effective but the other is not. In Table~\ref{table:rewrite_examples}, we first show an example when the ChatGPT rewrite is effective, whereas the participant rewrite is not. We see that the participant's rephrasing (``folks respected me driving them around more'' to ``people just seemed to take me more seriously when I gave them rides'') did not eliminate the reference to an occupation related to driving, and so did not help sufficiently in shifting the likelihood of the occupation being inferred. 
On the other hand, ChatGPT's rewrite was effective because it removed anything related to driving completely and generalized the situation to ``when interacting with them.''

The second example in Table~\ref{table:rewrite_examples} shows an instance where the participant rewrite is effective, but ChatGPT rewrite is ineffective in preventing the inference. ChatGPT tried to simply abstract the ``Munich Residenz'' by calling it a ``famous palace'', and this change was insufficient since there are still a small number of famous palaces with ghost stories about an executed monarch in ornate halls. However, the participant more cleverly abstracted the text by referring instead to ``local ghost story spot'', replaced ``monarch'' with ``man'', and eliminated the description of those halls being ``ornate.'' In this case, the participant was able to identify and hide more clues leading to the inference than ChatGPT, thus producing an effective rewrite.

Rescriber was able to produce more effective rewrites with its `replace' method because it removes the clues of the inference completely with a placeholder, while its `abstract' method might still leave some hints. For example, for an original comment ``Totally picked up photography after countless travel snaps by my wife!'', Rescriber replaced ``my wife'' with ``[NAME1],'' which successfully prevented the inference of the relationship status being married. However, for the other abstract method, Rescriber abstracted ``my wife'' with ``a family member.'' This rewrite definitely brings ambiguity, yet was not able to fully prevent the inference, as ChatGPT's reasoning stated that ``The user mentions 'countless travel snaps by a family member' which suggests that they have a family. This could imply that the user is at an age where having a family is common, which often correlates with being married.'' 

\subsubsection{Semantic Similarity}
%\input{fig/bertscore}
\input{fig/bert_fig_tab}
Mitigating a privacy risk often involves losses in utility; it is thus interesting to see what impact effective rewrites have on the semantic meaning of the original text. To measure how semantically similar the rewrites were to the original text, we calculated the F1 BERTScore for each rewrite-original text pair. \add{It is worth noting that BERTScore provides a measure of overall semantic similarity between two sentences, and is less sensitive to localized edits. It is not a direct measure of effectiveness.
%the resulting scores should not be interpreted as meaningful at the level of individual rewrites. 
Instead, we use BERTScore as an aggregate indicator of how much semantic content is altered across the dataset, rather than as a precise measure of sentence-level equivalence.}

Figure~\ref{fig:bertscore} shows a comparison of the overall distribution of scores, for each agent, of their effective rewrites vs. their ineffective ones. A first observation is that in both the effective and ineffective rewrites, the BERTScores are never low (usually above 0.9, and even outliers are greater than 0.8). This implies that no participant rewrote the original text with junk, and offers confirmation that our participants took the task seriously. Second, we see that effective rewrites exhibit lower F1 scores than ineffective ones, for each agent. This implies that overall, the effective rewrites moved further away in semantic meaning than the ineffective rewrites. However, care in interpreting this metric is important. BERTScores capture semantic similarity, but they do not capture privacy risk. It is possible in some cases to diminish the risk without changing the meaning of the original text much at all (depending on the case). The main takeaway from Figure~\ref{fig:bertscore} is that it is often possible to eliminate the privacy risk without incurring much loss on the semantic front. This is because the BERTScores for effective rewrites are still high. 

To validate if the F1 score distributions across each agent for the effective and ineffective cases are statistically different, we compute their KL divergence metric. Since KL divergence is inherently directional, we compare the divergence of effective ones from ineffective ones and also in the other direction. Table~\ref{table:kl_bertscore} shows the KL divergence values for all agents, which are all greater than 0.1 and even greater than 6 for agents other than our participants. These positive values greater than 0.1 confirm that the effective and ineffective F1 score distributions for each agent are statistically different. Although Rescriber achieved the highest average F1 score, it was the least effective at rewriting (see Table~\ref{table:rewrite_success}) with the largest differences in F1 scores. ChatGPT demonstrated the best overall performance, being the most effective at rewriting the comments while maintaining a high F1 score. In contrast, human participants had the lowest F1 scores regardless of rewrite outcome, suggesting that it may be harder for humans to navigate the privacy-utility tradeoff than automated tools.


%\input{tables/kl_berscore}

\subsubsection{Rewrite Strategies}
When we look at participant rewrite strategies, we notice that not all participants attempted to modify the text. Ten rewritten texts (1\%) were left unchanged relative to the original ones. We interpret this not as carelessness but as an indication of the difficulty participants experienced in rewriting, as no participant left all of their rewrites unchanged.

\input{tables/strategy_stats}
\input{tables/code_example}

Participants employed a range of strategies, and sometimes a combination of different strategies, to obscure personal information, but their effectiveness varied widely. As shown in Table~\ref{table:strategy_status}, the most common approach was paraphrasing/replacement (60\%), where participants reworded the text without altering its core factual content. Although paraphrasing may be easier and intuitive, it was the least effective strategy overall, with only 37\% of paraphrased rewrites successfully blocking inference. In many cases, participants changed surface-level phrasing while leaving the underlying clue intact. For instance, looking at the ineffective paraphrasing example in Table~\ref{table:code_example}, we can see that the participant has made some alterations to the text, but it did not change the meaning completely. On the other hand, in the effective paraphrasing case, the participant replaced the key term (`Shakshuka' dish), which was giving away clues to deduce the place of birth.

In contrast, more targeted strategies yielded higher success rates. Generalization/abstraction (19\%) was successful in nearly 67\% of cases. Similarly, omission/deletion (33\%) was effective in 63\% of cases. Although these strategies were less frequently used than paraphrasing, they were substantially more reliable. Less common strategies also showed promise. Misdirection (10\%) was effective 58\% of the time. Likewise, adding ambiguity (11\%), such as using vague descriptors, worked in 71\% of cases. Though these strategies were less common, their higher effectiveness suggests potential strategies for privacy-preserving rewriting. 

%\add{Figure~\ref{fig:bertscore_strategy} shows the BERTScore F1 distributions for effective and ineffective rewrites across all strategies. Echoing our previous finding, effective rewrites consistently exhibit lower semantic similarity to the original text than ineffective ones. This pattern holds across all strategies, indicating that rewrites that introduce more substantial semantic changes are more likely to prevent attribute inference.
%Additionally, the figure highlights differences in how much semantic content each strategy tends to alter. Paraphrasing produces the most varied F1 scores overall, reflecting the wide range in how participants applied this strategy, from replacing a single term to rephrasing entire sentences. Correspondingly, paraphrased rewrites show the largest gap between effective and ineffective cases in terms of similarity. In contrast, strategies such as omission/deletion and generalization/abstraction exhibit higher F1 scores when effective, consistent with their more targeted removal or broadening of specific information rather than rewriting the entire sentence structure.}

Table~\ref{table:code_example} shares an effective and an ineffective example for each of these strategies. Some of these rewrite strategies are clear; however, others are more subtle. In the ineffective example for generalization/abstraction, the original text makes it clear that the person is talking about themselves, while the rewrite implies they are talking about society in general. In the ineffective example for adding ambiguity, the participant garbled the text, making it hard to parse. In the effective misdirection example, the original text implies the person has a hard time sorting laundry, while the rewrite simply states that at that moment, the person has finished the task. The notion that sorting laundry might be challenging is lost, making this rewrite successful. This example is essentially the opposite of an abstraction case, as the participant has removed a general stereotype to pinpoint the completion of an action at a specific moment.

\add{Compared to prior work examining untruthful disclosure~\cite{truth2021}, the strategies observed in our study reflect a fundamentally different mode of privacy protection. In our rewrite task, participants consistently worked within the original text, altering phrasing or detail while preserving coherence and overall meaning. Even when aiming to obscure an attribute, they treated the text as something to be improved or reshaped, not completely rejected. By contrast, participants in~\cite{truth2021} often abandoned fidelity to the question entirely, relying heavily on categorical strategies such as refusing to answer, supplying invalid information, or providing fully fabricated responses when faced with personal questions. These strategies were especially prevalent for highly sensitive or constrained items, such as passport numbers, credit card digits, or social security numbers, where participants opted to break format or withdraw.

These contrasting patterns highlight that the structure of the task strongly shapes the space of privacy-preserving behavior. A rewrite task without explicit personal information encourages meaning-preserving edits and subtle linguistic obfuscation, pushing participants toward strategies that manipulate how information is expressed rather than whether it is provided. In a direct personal information setting, however, the primary decision is whether to disclose at all, leading participants to favor coarse-grained refusals, fabrications, or invalid responses. These results suggest that privacy-preserving actions are highly context-dependent.}
