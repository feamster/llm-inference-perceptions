\section{Conclusion}
Our study examined how users estimate and respond to inference-based privacy risks in LLM interactions. We found that participants were able to reasonably estimate inference risk for location and relationship status, consistently missed the risk for occupation, and did no better than random for the other attributes. Participants employed a variety of rewriting strategies ranging from paraphrasing to misdirection. The most common approach of paraphrasing, while intuitive, turns out to be a weak strategy, while less common strategies, such as omission, generalization, and adding ambiguity, were more effective. When compared with automated tools, user rewrites were moderately more effective than Rescriber but substantially less effective than ChatGPT. However, both Rescriber and ChatGPT were able to preserve the meaning of the original text better than our participants when their rewrites were effective at blocking the inference. These findings highlight a gap between recognizing inference risks and taking effective action. Addressing this gap requires design interventions that make the risks visible, support more effective rewriting practices, and provide system-level protections that reduce the burden on users. Our study contributes to the future design of inference-aware systems that protect privacy while empowering users to engage with LLMs with greater confidence and agency.