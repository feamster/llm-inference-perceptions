\section{Method}
To study our research questions, we designed a survey to collect user perceptions towards private attribute inference. Our institution's Institutional Review Board (IRB) approved this study, and all participants provided informed consent prior to participation. None of the participants contacted the IRB at the provided email address to request withdrawal from the study or data deletion.

\subsection{Recruitment and Participants}
We recruited participants through Prolific\footnote{https://www.prolific.com}, a crowd-sourcing platform for research studies, in June and July 2025. We required participants to be over 18, fluent in English, and residing in the United States. Except for requesting a balanced gender sample, we did not set any criteria in Prolific for other demographic factors. The survey took about 20 minutes to complete, and we compensated participants \$6 upon completion, resulting in an average compensation of \$18/hour. \add{We conducted two checks to ensure data quality; first we automatically flagged responses with repeated selection of the same choices (for multiple choice sections) and second, we manually verified all text entries to ensure they made sense. These text responses also served as our attention check questions. In total, nine responses were excluded from the final analysis.} The final dataset contains 240 responses, which came from 114 females, 121 males, and five non-binary participants. Details on demographic information of our participants can be found in Table~\ref{table:demographic}.

\input{tables/demographics}

\subsection{Survey Design}
\subsubsection{Dataset}
\label{sec:dataset}
As shared in Section~\ref{sec:rel_attribute_inference}, we leveraged the SynthPAI synthetic dataset for personal attribute inference exploration~\cite{synthpai}. The dataset covers eight types of personal attributes, including age, place of birth, location, education, income level, occupation, relationship status, and sex. \add{The selection of these attributes was inspired by datasets created by the American Census Bureau, and are sensitive attributes as shown in other studies~\cite{li2020, lee2021digital, zhang2025through}} For each inference, the dataset also provides a hardness level on a scale of 1 (easy) to 5 (hard), indicating how hard it is to infer the attribute from the text. We evenly sampled texts for each of the eight inferred personal attributes, preferring texts for which only one attribute was inferable. Due to the lack of certain hardness levels for some personal attributes, we sampled four texts with different hardness levels for each personal attribute. 
%Additionally, we sampled four comments with different hardness levels where no personal attribute is inferable by LLMs. 
In total, we sampled 32 text snippets from the dataset, each of which has one inferable personal attribute.

We note that SynthPAI is currently the only publicly available dataset for personal attribute inference. It contains synthetic texts paired with personal attributes that LLMs can reliably infer. Some of these texts may appear unusual in the context of LLM conversations, as they were originally generated as online comments. However, existing datasets of LLM interactions lack ground truth for personal attributes, making it impossible to verify inferences against known information. Collecting real user conversations along with personal attributes would also raise significant privacy concerns, making SynthPAI the most practical and ethical choice for our study.

\subsubsection{Survey Questions}
\label{sec:survey_questions}
To ensure the validity of our survey, we conducted two rounds of pilot testing. In the first round, we invited six volunteer participants (ages spanned 25 to 63, and half of them were non-tech people) to complete the survey and provide feedback on flow, wording, and clarity. Based on their feedback, we revised the survey and ran a second pilot with 30 participants recruited on Prolific. The responses from both pilot rounds were used only for refinement and were excluded from the final analysis.

We made two major revisions as a result of the pilots. First, participants in the first round reported that they did not fully understand the task until completing the first question set, and some believed they had answered incorrectly. To address this, we added a practice question set (see Appendix~\ref{sec:practice_set}) with example answers at the beginning of the survey. This set was included solely for the purpose of illustration and was excluded from the final analysis. Second, in the second pilot, we measured concern levels both before and after revealing the target attributes. However, we found it difficult to interpret changes in concern (e.g., teasing apart the influence of participants' over/under-estimations was very complex), so we omitted the initial concern question from the final survey.

For each sampled text and its target attribute pair, we constructed a question set (see Appendix~\ref{sec:question_set}). We first asked participants to estimate how likely each of the eight attributes could be inferred from the text on a 5-point Likert scale of `Very unlikely' to `Very likely'. Although only one attribute was actually inferable from each text, we did not provide participants with this information and allowed participants to select more than one attribute. We then revealed the target attribute inferred from the text to the participants, and asked them to rate (on a 5-point Likert scale of `Not at all concerned' to `Extremely Concerned') how concerned they were about including this text in their conversation with an AI-powered chatbot. Finally, we asked participants to try their best to rewrite the text so that the target attribute could no longer be inferred by an LLM. Effectively, each question set has three questions that are all related to the same text and its personal attribute pair.

We first had each participant go through a practice question set with an example rewrite and explanation to ensure their understanding of the task. They then answered four question sets we randomly selected from our question set pool, ensuring that each participant saw texts related to four (out of eight) different inferred target attributes. We ended the survey with questions on their general AI chatbot usage (``How frequently do you use any AI-powered chatbots?'')
%, whether they knew about personal attribute inference from texts before taking this survey (``Did you know, before taking this survey, that an AI chatbot could possibly figure out personal information not explicitly shared in the text?'') and if so, how, 
followed by demographic questions. We include the complete survey in Appendix~\ref{app:survey}.



\subsection{Data Analysis} 
\subsubsection{User Estimation} \label{sec:user_estimation_methods}
To evaluate the accuracy of users estimating which personal attributes were inferable, we calculated a score and a weighted score for each participant as follows.

%For any text and attribute pair, we considered `Very likely' and `Likely' responses from the participant as positive estimation, and `Very unlikely' and `Unlikely' as negative estimation. %The estimation for an attribute is considered accurate if it is positive and the attribute is the target attribute for the text, or negative for non-target attributes.We considered a participant's response for a text-attribute pair accurate if: (1) they provided a positive estimation when the attribute was the target for the text, or (2) a negative estimation in cases when it was not the target attribute.

\add{For a given text and attribute pair, there are 2 ways the user can make an accurate prediction. If the participant selects 'Very Likely' or 'Likely'  that the attribute is inferable from the text, and the SynthPAI label say it is inferrable, then the user's positive estimation is correct. Similarly, if the participate selects 'Very Unlikely' or 'Unlikely' and SynthPAI says the attribute is not inferable, then the user's negative estimation is accurate.}

%%The \textit{score} for a user reflects how many accurate positive estimations were made for target attributes, regardless of their estimations for non-target attributes. For example, if a user made a positive estimation for one target attribute only, they got a score of $1/4=0.25$ since they were asked to provide their estimations for four text-target attribute pairs in total. 

\add{The \textit{score} for a user reflects how many of the inferable attributes they estimated were inferable (among their set of questions). For example, if the a user's questions include 4 cases of attributes that were indeed inferable from their text, and the user only correctly estimated that one of them was inferable, then their score would be $1/4$. In other words, this captures what fraction of the true positive examples shows to a participant they estimate correctly.} Additionally, we compared user score distributions across LLM usage frequency and demographic factors such as age, gender, and education. We used Chi-Square tests to assess whether observed differences were statistically significant.

%The \textit{weighted score} for a user reflects how many accurate estimations they make across all targeted and non-targeted attributes. The first part of the score is calculated based on the estimation for each target attribute being accurate or inaccurate, yielding a score of 1 or 0 respectively. The second part of the score calculates how many of the non-target attribute estimations are accurate out of all non-target attribute estimations. We then normalize the sum of the two parts. To demonstrate, consider a user who makes a positive estimation for the target attribute and four non-target attributes, and negative estimations for the remaining three non-target attribute. It follows that their estimations are accurate for the target attribute and three non-target attribute with negative estimation, and inaccurate for the four non-target attributes with positive estimations. Then, for this question set the user gets a weighted score of $0.5(1+3/7)=0.71$. If the user made a negative estimation for the target attribute and all their remaining estimations are the same, then the user gets a weighted score of $0.5(0+3/7)=0.21$. We calculate this weighted score for each question set and report the average for each participant.

The \textit{weighted score} for a user reflects how many accurate estimations they make across all \add{inferrable and non-inferrable attributes}. The first part of the score is calculated based on the participant's estimate of whether the target attribute can be inferred. \add{(Recall we use the term \textit{target} attribute to refer to the attribute that can indeed be inferred, and \text{non-target} to refer to attributes that cannot be inferred.)} They get a 1 if they predict it can be inferred, or a 0 otherwise. This captures whether they can identify a true positive. For each sample text, there are 7 non-target attributes that cannot be inferred. A participant's prediction is correct if they guess that these non-target attributes are not inferable (true negative). They get 1/7 of a point for each true negative. The total score is thus $\frac{1}{2} ( \frac{\{0,1\}}{1_{TP}}) + \frac{num_{cn}}{7_{TN}})$ where $TP$ refers to True Positive, $TN$ is True Negative and $cn$ refers to correctly guessed negatives. (This notation simply conveys that there is only 1 example in the denominator for true positives, and 7 examples in the denominator for true negatives - for a given question set.) For example, if a participant correctly predicts the target attribute and correctly estimates 3 out of the 7 non-target attributes, then that participant gets a score of $0.5(1+3/7)=0.71$. If the participant incorrectly estimates that the target attribute is not inferable, but correctly estimates 3 out of 7 non-target attributes, then their score is $0.5(0+3/7)=0.21$. This weighted score places greater emphasis on correctly identifying the inferable attribute, while also giving credit for correctly ruling out attributes that are not inferable. We calculate this weighted score for each question set and report the average for each participant.


\subsubsection{Rewrite Evaluation}
We evaluated the participants' ability to effectively rewrite texts for inference prevention from two aspects. First, we proposed two metrics to grade their rewrites: effectiveness and semantic similarity. To measure the effectiveness of a rewrite, we replicated the inference pipeline as described in~\cite{synthpai} and examined whether the target attribute was still inferable from the rewritten text. \add{Specifically, the prompt asks the LLM to guess which personal attributes can be directly or indirectly inferred from the given text. We used GPT-4 (version gpt-4-0613) as our LLM, which demonstrates the highest performance, in combination with the `Model Tagging Prompt' described in Appendix D.4 of ~\cite{synthpai}.NT: shouldn't we add that this is the version usedin SynthaPAI paper?}

To measure the semantic similarity of the rewrite to the original text, we employed BERTScore~\cite{bertscore}. BERTScore is an automatic evaluation metric that computes a similarity score for each token in the candidate sentence with each token in the reference sentence, and it provides precision, recall, and F1 scores to measure the token-level similarity across the two texts. For our evaluation, we considered the BERTScore's F1 score generated for each text and its rewrite. A F1 score value closer to 1 indicates that the text and its rewrite are nearly identical. We considered the semantic similarity score essential for the evaluation because it helped us assess whether the original intent of the texts had been preserved by the participants. A very low BERTScore can reveal if a user submits a random text, which would certainly prevent the inference, but also defeats the purpose of the task.

Second, we also benchmarked participants' performance with that of online tools, namely ChatGPT and Rescriber~\cite{rescriber}. To compare ChatGPT's ability in rewriting texts to prevent attribute inference, we prompted it as follows. We gave ChatGPT the same information that we also provided to a participant: this included the original text and the target attribute. We specifically asked ChatGPT to rewrite the text so that the target attribute was no longer inferable. The template of the prompt can be found in Table~\ref{table:rewrite_prompt}. Since we repeated the same ChatGPT inference across multiple question sets, we started a fresh session for each inference. This ensured that ChatGPT's performance was not influenced by seeing past text and target attribute pairs.

\input{tables/gpt_prompt}

Rescriber~\cite{rescriber} is a state-of-the-art data minimization tool that detects and suggests sanitizing options for removing personal information in user prompts. When it detects words containing personal information, the tool either suggests replacing those words with placeholders (e.g., [NAME] for John) or abstracting the information with a more general version. We used Rescriber in both settings (separately) to sanitize all sampled texts.


\subsubsection{Understanding user rewrite strategies}
To investigate the strategies users employed when rewriting text to prevent attribute inference, we conducted an open-coding analysis. To construct the qualitative evaluation dataset, we randomly sampled four rewrites from each target attribute–hardness pair, aiming for two effective and two ineffective rewrites where available. This resulted in 124 samples for analysis.
%(some attribute-hardness pairs did not have success rewrites at all, and therefore had fewer than four rewrites in the dataset). 

Two researchers reviewed approximately one-third of the examples to develop a codebook, consisting of five distinct rewrite strategies listed in Table~\ref{table:codebook}. Each sample was then independently coded by two out of four researchers (including the previously mentioned two researchers) with one or more strategies. \add{We assessed inter-rater reliability using Krippendorff’s $\alpha$ (nominal), which yielded $\alpha = 0.79$, indicating substantial agreement between annotators.}
Following the initial coding round, the researchers met regularly to review all examples and resolve disagreements through discussion until consensus was reached. 
\input{tables/codebook}


\subsection{Limitations} \label{sec:limiation}
Our study has several limitations.
We relied on text snippets from the SynthPAI dataset~\cite{synthpai}, which provided an established pipeline and ground truth of personal attribute inference, but may not fully capture the richness of real-world information leakage. Additionally, participants completed the tasks in a survey setting, where they were encouraged to overestimate, and their reported concern levels and motivation for rewriting may not reflect real behavior during personal chatbot conversations with higher stakes or social pressure. Therefore, participant scores, concerns, and rewrite abilities should be carefully interpreted. Our survey design also introduced a privacy priming effect. After the first practice question set, participants became aware that the study was about privacy and that risks existed. Thus, their concern ratings reflected awareness of potential risk rather than naïve perceptions. While this may limit real-world relevance, our research still provides a valuable perspective by revealing participants who remained unconcerned even when privacy risks were made explicit. In addition, our comparisons with ChatGPT were based on specific and simple prompts, and therefore, results may vary with prompt engineering or future system updates. 

Generalizability is also constrained. Our sample consisted of 240 U.S. adults, limiting applicability across cultural and linguistic contexts. While our use of synthetic data avoided exposing any real personal information, participants may not have the same personal connection with the text as with genuine text from conversations, since the text snippets were synthetic and did not reflect participants’ own information. As a result, some participants may have taken the privacy risks less seriously than they would if the information had reflected their actual circumstances. Future work may extend this research to more real-world relevant, longitudinal, or cross-cultural settings, and explore how interface design and contextual cues shape whether users notice, care about, and act on inference-based privacy risks.

