\section{Related Work}
In this section, we review literature on LLM privacy vulnerabilities, personal attribute inference capabilities, and user perceptions of AI privacy risks.

\subsection{LLM Privacy and Security}
Commonly published large language models (LLMs) are frequently trained on both public and private datasets, and are susceptible to popularly known membership inference attacks~\cite{shokri2017membership}. Specifically, prior research has demonstrated that it is feasible to extract verbatim examples~\cite{carlini2021extracting}, sentence fragments~\cite{inan2021training}, canaries~\cite{Parikh2022}, or even ngrams~\cite{mccoy-etal-2023-much} from the training data. Even benign updates to natural language models have been shown to leak changes in the training data~\cite{zanella2020ccs}, and many techniques and toolkits have been proposed to improve the success rates of these attacks~\cite{carlini2022membership,carlini2023quantifying,mireshghallah-etal-2022-quantifying, li2024vldb}. %Toolkits, such as LLM-PBE~\cite{li2024vldb}, have been created for systematically evaluating data privacy risks in LLMs across their lifecycle, focusing on unintentional training data leakage. %They shed light on influential factors such as model size, data characteristics, and evolving temporal dimensions.

LLMs are also prone to other adversarial attacks like prompt hacking, which manipulate input prompts to obtain desired and sometimes unintended or malicious outputs~\cite{crothers2023machine}. These can be further classified into \textit{Prompt Injection} or \textit{Jailbreaking} attacks~\cite{das2025security}. In prompt injection, the attacker aims to misalign the goal of original prompts or recover information from private prompts~\cite{perez2022ignore}. On the other hand, jailbreaking prompts aim to bypass (safety) restrictions, say to generate malicious, toxic, harmful, or offensive content~\cite{liu2023jailbreaking}. 

Both memorization and prompt hacking techniques have been extensively leveraged to extract personal attributes, including PII, from LLMs. Prior research has demonstrated that private information can be extracted by simply querying LLMs~\cite{lukas2023analyzing, li-etal-2023-multi-step}. In fact, it has been shown that it is feasible to extract names and email addresses even if they appear only once in the training data~\cite{carlini2021extracting}. However, this prior research on PII leakage from LLMs has certain shortcomings: they either focus on extracting specific PII types (e.g., email addresses~\cite{huang-etal-2022-large} or phone numbers~\cite{nakka2024pii}), are specific to one domain (e.g., biomedical~\cite{nakamura2020kart}), or are evaluated on small transformer-based language models~\cite{Siwon2023propile, inan2021training}.

\subsection{Personal Attribute Inference by LLMs}
\label{sec:rel_attribute_inference}
Beyond memorization and PII extraction, recent research has shown that LLMs' inference capabilities can be leveraged to infer personal attributes (e.g., location, age, income, sex) from texts~\cite{staab2024beyond}. While author profiling or private attribute predictions from digital records has been a long-standing area of classical natural language processing (NLP) research~\cite{estival2007author, kosinski2013private}, LLMs help improve accuracy and conduct inference at scale~\cite{weidinger2021ethical, brewster2023chatgpt}. Also, LLM's multi-modal capabilities further extend the attribute inference to even images~\cite{tomekcce2024private}.

Online self-disclosures, while being rewarding in social media interactions, contribute to these personal attribute inferences~\cite{dou-etal-2024-reducing}. These (unintended) personal or sensitive disclosures have also been observed in the context of conversational agents~\cite{mireshghallah2024trust,zhang2024s}. LLMs' ability to employ persuasive strategies and exhibit anthropomorphic characteristics increases user trust~\cite{cohn2024believing} and the likelihood that users will disclose more personal information than they intend~\cite{ischen2019privacy,stock2023tell}. \add{Experiments manipulating an AI systemâ€™s name, style, or persona show that anthropomorphism increases user reliance and sensitive disclosure~\cite{bi2023create}. Anthropomorphic chatbots also reduce privacy apprehension by making interactions feel more reciprocal~\cite{shim2024unveiling, jo2024understanding}, and recent LLM studies find that empathetic or human-like models elicit more detailed disclosures even from privacy-concerned users~\cite{choi2025privacy}. At the same time, broader analyses caution that anthropomorphism can heighten over-reliance and privacy risks~\cite{peter2025benefits}.} While users sometimes manually sanitize their inputs to avoid revealing sensitive information~\cite{zhang2024s}, they fall short due to tediousness. To address this, browser extensions have been developed to detect and highlight potential personal information disclosures, so that users can redact or abstract these before sending their messages~\cite{rescriber}. While such sanitizer solutions help prevent explicit disclosures in the text (e.g., person names, city names), they are not very effective against implicit disclosures~\cite{synthpai} (e.g., inferring city name from unique landmarks), as we also show later in this work.

Current work specifically focuses on implicit private attribute inference from text inputs. There is a lack of available real-world datasets for LLM attribute inference and defense research~\cite{staab2024beyond}. %, as prior publications do not release their datasets due to privacy concerns.
This has motivated some to generate synthetic datasets, such as SynthPAI~\cite{synthpai}, which we also utilize in our research. 


\subsection{User Perceptions of AI and Privacy}
User perceptions when interacting with conversational chatbots have been
extensively studied in the usable privacy literature (e.g.,
~\cite{chalhoub2020alexa, chametka2023security, belen2021privacy}). Most of
these are on pre-LLM chatbots, and there is limited user research on LLM-specific
conversational agents, most of which are small-scale qualitative interview
studies~\cite{kimbel2024security,zhang2024s} or one-country focused
surveys~\cite{malki2025hoovered,liu2025prevalence}. These studies have shown
that users are primarily concerned about the collection and storage of chat
interactions~\cite{belen2021privacy,chametka2023security,kimbel2024security,zhang2024s}
and service providers' ability to create personalized
profiles~\cite{liu2025prevalence,rescriber}. Users have also expressed unease
around LLMs' unauthorized data sharing and cyber
attacks~\cite{alawida2024unveiling, alkamli2024understanding}. 

\add{Users often exhibit the privacy paradox where they voice strong privacy concerns yet disclose sensitive information when convenience, utility, or conversational ease outweigh perceived risks. This pattern extends to LLM-based agents, where users frequently accept disclosure risks because models feel helpful, low-friction, or benign~\cite{zhang2024s, mildner2024listening}. At the same time, many users misunderstand how LLMs operate and have limited awareness of inference risks, which constrains their ability to give informed consent or protect themselves effectively~\cite{DSIT2024PublicAttitudes, kimbel2024security, zhang2024s, rescriber}. Prior works have also found that people worry about generative AI privacy but rarely take protective actions, citing uncertainty, resignation, or optimism about model behavior~\cite{malki2025hoovered, kwesi2025exploringusersecurityprivacy}. Together, these dynamics help explain why users may inadvertently disclose information that enables attribute inference.}
%Furthermore, Zhang et al.~\cite{zhang2024s} found that users held erroneous mental models of how conversational agents like ChatGPT generate responses (e.g., assumed LLMs work like search engines), and they did not understand LLM-specific risks such as memorization. In another study, Malki et al.~\cite{malki2025hoovered} found that engagement with privacy protective controls offered by LLMs was overall low, and that many participants held mismatched expectations of opting out of model training and the effects of deleting data. This lack of awareness of privacy risks and misunderstandings around how LLMs operate technically limits their ability to give informed consent and protect themselves against privacy risks~\cite{DSIT2024PublicAttitudes, kimbel2024security, zhang2024s, rescriber}. 


\subsection{Summary}
While prior work has established that LLMs can infer private attributes and has explored general user perceptions towards privacy when using conversational agents, no research has directly explored user perceptions towards implicit private attribute inference. Our work is the first to systematically investigate whether users can estimate the inference of eight different private attributes from innocuous texts, how concerned they are once made aware of the inference, and if users are able to effectively rewrite texts to avoid the attribute inference. 
