\section{Introduction}
Large Language Models (LLMs) such as ChatGPT have become everyday companions in recent years. By April 2025, ChatGPT alone reached an estimated 800 million weekly active users with a daily average of 187.91 million visits~\cite{ChatGPTStats}, reflecting its deep integration into various aspects of people's lives. Such scale underscores that interactions with LLMs are no longer exceptional events but routine parts of everyday digital life, shaping how people communicate and make decisions.

With this normalized usage comes great concerns around privacy and trust. HCI research has long documented how users weigh the benefits of disclosure against the risks of data collection in digital systems. For LLMs, these risks are further amplified by memorization and reproduction of sensitive content from training data~\cite{carlini2021extracting, inan2021training}. To address this, various techniques and tools were developed for Personally Identifiable Information (PII) detection and sanitization that target explicit identifiers such as names, emails, and phone numbers~\cite{huang-etal-2022-large, nakka2024pii, rescriber}. While these techniques and tools enhance privacy protection, they target only what users type in, not what the model can infer.

In this work, we emphasize a different class of PII leakage: inference-based privacy risk. Unlike memorization of PIIs, which requires models to remember sensitive data explicitly mentioned in conversations seen during training, inference allows models to deduce personal information from everyday conversations that are seemingly unrelated to personal information. A casual mention of weekend activities, a favorite restaurant, or workplace jargon may allow an LLM to accurately guess a userâ€™s age, location, occupation, or even relationship status~\cite{staab2024beyond, weidinger2021ethical, brewster2023chatgpt}. Unlike PII leaks, these risks are difficult for users to notice and challenging to redact once shared, since the sensitive information was never explicitly disclosed in the first place.

Prior work suggests that users are poorly prepared for these risks. Studies of LLM privacy perceptions show that users tend to focus on data collection and storage~\cite{belen2021privacy,chametka2023security,kimbel2024security,zhang2024s} or fear the creation of personalized profiles by service providers~\cite{liu2025prevalence,rescriber}. Users also often hold incomplete or inaccurate mental models of LLMs, sometimes treating them as search engines or databases rather than predictive systems~\cite{zhang2024s, malki2025hoovered}. This mismatch makes users poorly equipped to anticipate inference risks or to take effective action. Moreover, the conversational qualities of LLMs, such as their fluency, persuasiveness, and anthropomorphic design, make people more willing to self-disclose, sometimes beyond what they meant to disclose~\cite{ischen2019privacy,stock2023tell}. Together, these dynamics create a widening gap between what LLMs can know and what users expect them to know, raising critical questions about transparency, trust, and user agency.

To bridge this gap between what LLMs can infer and what users expect them to infer, we ask the following research questions:
\begin{itemize}
    \item RQ1: To what extent do users realize what personal information can be inferred by LLMs?
    \item RQ2: Do users have different concern levels depending on the type of information that can be inferred?
    \item RQ3: Can users effectively rewrite text to prevent inference? If so, what strategies do they use?
\end{itemize}

To address these questions, we conducted a survey study with 240 U.S. participants. Each participant was shown short text snippets drawn from SynthPAI, a synthetic dataset designed for personal attribute inference research~\cite{synthpai}. For each snippet of text, participants were asked to (1) estimate which personal attributes could be inferred, (2) report their concern levels once the inference was revealed, and (3) attempt to rewrite the text to block inference while preserving the original meaning. To evaluate participant rewrites, we compared them with rewrites generated by ChatGPT and Rescriber, a state-of-the-art sanitization tool~\cite{rescriber}. We also analyzed user rewrites to identify common strategies employed (e.g., omission, generalization, and adding ambiguity). Together, this mixed-method approach of both quantitative analysis of survey responses and qualitative analysis of participant rewrites allows us to evaluate not only whether users can accurately estimate the inference risks, but also whether they could meaningfully act on them.

Our findings reveal several key insights. We find that users struggled to anticipate which attributes could be inferred, performing only slightly above chance. Nearly half of the participants expressed concern once the inference was revealed, though concern levels did not differ strongly across attribute types. Finally, we show that while users attempted a range of rewriting strategies to block inference, they were only partially effective and less successful than ChatGPT.

Our work provides empirical evidence of user estimates and concerns towards LLM personal attribute inference, and a thorough analysis of user rewrite strategies and the effectiveness of user-written and tool-generated rewrites. The implications of our study extend beyond abstract concerns of privacy. Inference-based risks affect how people interact with LLMs, in ways from workplace assistants that may reveal employee status, healthcare chatbots that may infer sensitive medical information, to educational tutors that may profile students. By clarifying how users perceive, misperceive, and attempt to mitigate inference, our work provides a basis for designing inference-aware safeguards that balance usability with protection in real-world LLM applications.

