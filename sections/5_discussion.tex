\section{Discussion}
Our findings provide important insights into how users estimate and respond to the risk of attribute inference from text, as well as how different rewriting strategies mitigate that risk. In this section, we discuss the broader implications of our findings and potential future work.

\subsection{User Estimation and Concerns}
\label{subsec:discuss-awareness}
As captured in Section~\ref{sec:user_estimation}, participants tended to overestimate which attributes are inferable, yet their accuracy remained limited and only modestly above chance. This miscalibration is consistent with recent works showing that users hold incomplete or inaccurate mental models of LLM capabilities~\cite{li2024humancenteredprivacyresearchage, windl2022contextualprivacy}. It is worth noting that asking participants to guess which attributes were inferable naturally encouraged them to seek patterns and overselect. In everyday use, however, users are unlikely to reason deliberately about what can be inferred from their text, echoing findings in privacy research showing that users often struggle to anticipate information flows \cite{malki2025hoovered, zhang2024s}. Additionally, user performance varies significantly, with 8\% making accurate estimates for all four questions, 14\% making no accurate estimates, and a larger variance in rewrites than automated tools (Figure~\ref{fig:bertscore}). Therefore, privacy protection cannot depend on user vigilance, and systems need to provide proactive, inference-aware support.

\add{More than 70\%} of our participants expressed concern about at least one attribute being inferred, yet concern levels did not vary significantly by attribute type. Because participants were evaluating attributes that did not describe themselves, their judgments lacked personal stakes, which might lead to muted responses. Prior work has shown that privacy concerns are highly contextual, potentially increasing when consequences are concrete or personally meaningful \cite{nissenbaum2004privacy}, and often appear attenuated when risks are presented abstractly or not self-related~\cite{WOODRING2024103997}. We therefore consider our results as a lower bound on user concern. We reiterate that we used SynthPAI because it was publicly available and was a previously vetted dataset. We also point out that doing studies with examples that are personal to the participants can be challenging to set up and intrusive to participants' privacy. When attributes are personally relevant or when potential harms are made apparent, concern levels are likely to be substantially higher.

\subsection{Defense Mechanism against Attribute Inference}
Our comparison of different rewrite agents highlights the limitations of both human cognition and current sanitization tools. Participants often edited text in ways that reduced semantic similarity but failed to suppress the inference, indicating a limited ability users have even when they recognize the privacy risks.
Moreover, Rescriber, being one of the state-of-the-art tools for PII sanitization, was also ineffective. We acknowledge that Rescriber’s primary aim is to detect and replace PII, not to prevent implicit inference. Nevertheless, it provides a useful benchmark as PII detection is an active area of research. Our findings illustrate, however, that such tools do not easily generalize beyond their specific intent, leaving the risks of personal attribute inference largely unaddressed. Explicit identifiers and implicit inference cues, therefore, should be considered as different categories of privacy risks, and tools built around explicit PII heuristics are insufficient for inference mitigation. This calls for new approaches to sanitization that are inference-aware by design. Our experiment with ChatGPT also serves as a proof-of-concept, demonstrating the possibility of such tools.

 
Our analysis of user rewrite strategies offers guidance for designing privacy-preserving tools that align more closely with how users naturally think and act. \add{One potential explanation for differences in strategy effectiveness is that effective rewrites tend to modify or remove the textual elements that supported the inference, whereas ineffective rewrites often leave those elements intact.} Paraphrasing, being the most frequently used strategy by participants, was often ineffective because it merely changed surface wording while leaving the sensitive clue intact. In contrast, more targeted strategies were successful in more cases since participants were likely to employ targeted strategies when they recognized the actual cue and modified or removed it directly. This suggests that inference-aware sanitization tools must first be able to accurately detect inference cues. Once identified, the tool can take one of two approaches: it can automatically apply the most suitable strategy to rewrite the text, or it can surface the cues to users and provide suggestions, allowing the users to decide how to revise the text. \add{While some ineffective rewrite strategies may still reduce inference risk (by making it harder to infer) without fully eliminating it, we did not analyze partial increments in hardness levels. Replicating the SynthPAI hardness annotation process is not feasible because the original hardness levels were manually assigned and the protocol for doing so is not publicly available. %The `model tagging prompt' produced hardness levels cannot be applied since its predictions do not directly correspond to the manual hardness labels. 
The hardness predictions from the ‘model tagging prompt’ used in the original paper do not have a direct mapping to the hardness levels manually assigned in the dataset, thus we cannot use the prompt either to measure hardness of the rewrites.
Given these constraints, we focus on the binary outcome of whether an attribute is inferred, which is ultimately the most consequential in real-world settings. Although increasing the difficulty of inference can offer some protection, it does not remove the underlying risk.}

\add{Beyond technical performance, inference-aware sanitization tools introduce important ethical considerations. Automated rewriting systems may reduce disclosure risks, but they also raise questions about user agency and whether users remain meaningfully in control of what is revealed and how their voice is represented. Additionally, systems that intervene on user text can inadvertently introduce new privacy risks, such as retaining sensitive content for processing or generating sanitized outputs that misrepresent user intent. These tensions highlight the inherent tradeoff between privacy and usability, that stronger protections often require more intrusive transformations, additional interaction steps, or reduced expressiveness, while highly usable systems may provide insufficient defense against inference risks. Designing inference-aware sanitization tools therefore requires balancing these competing values and ensuring transparency about how interventions operate so that users can make informed decisions about their disclosures.}


\subsection{Future Research}
This study presents several avenues for future research. Our use of synthetic text enabled controlled measurement of inference risks, but future work should examine how users perceive and respond to inferences from their real-world conversations, where disclosures involve genuine personal stakes. Additionally, while our rewriting tasks provide insight into user strategies, in-situ or longitudinal studies of actual disclosure behavior are needed to understand how users react in the flow of everyday interactions with LLMs. Given that user profiling generally happens over time across multiple interactions with an LLM, longitudinal studies are necessary to demonstrate the utility of preventing implicit attribute inference. One promising direction to address both these aspects is an experience sampling study where participants install a browser plugin to surface implicit inference warnings, similar to what has been shown feasible in \cite{rescriber}.

Finally, future research should move beyond measurement to design and evaluate interventions such as interactive warnings, rewriting suggestions, or automated sanitization that help users identify inference risks and take effective action. Preferably such enhancements should be integrated into the LLM tool itself to avoid barriers for adoption. Advancing this work can bridge technical approaches with human-centered design to create inference-aware systems that both reduce risk and preserve the usability and utility of user-LLM interactions.